{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "### Introduction\n",
    "\n",
    "For this project, you will work with the [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) environment.\n",
    "\n",
    "![SegmentLocal](robotic_arms.gif \"segment\")\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project utilised the DDPG (Deep Deterministic Policy Gradient) architecture outlined in the [DDPG-Bipedal Udacity project repo](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal).\n",
    "\n",
    "## State and Action Spaces\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector must be a number between -1 and 1.\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "The agent training utilised the `ddpg` function in the `Continuous_Control` notebook. It continues episodical training via the the ddpg agent until `n_episoses` is reached or until the environment tis solved. As above, a reward of +0.1 is provided for each step that the agent's hand is in the goal location. The DDPG agent is contained in `ddpg_agent.py`.\n",
    "\n",
    "### DDPG Hyper Parameters\n",
    "\n",
    "Hyperparameters used are:\n",
    "\n",
    "- Replay buffer size = 1e6\n",
    "- Minibatch size = 128\n",
    "- Discount factor = 0.99\n",
    "- Tau for soft update of target parameters = 1e-3\n",
    "- Learning rate of the actor = 1e-4\n",
    "- Learning rate of the critic = 1e-4\n",
    "- L2 weight decay = 0\n",
    "- Number of episodes = 300 \n",
    "- Maximum time per epsode = 1000\n",
    "- Number of agents = 1 (or 20 based on the chosen environment)\n",
    "- Number of actions = 4\n",
    "- Number of states = 33\n",
    "- Number of learn updates in memory samples = 10\n",
    "- Number of time steps required to start learning again = 20\n",
    "\n",
    "Actor and Critic network models were defined in 'ddpg_model.py'. The Actor networks utilised two fully connected layers with 256 and 128 units with relu activation and tanh activation for the action space. The network has an initial dimension the same as the state size. The Critic networks utilised two fully connected layers with 256 and 128 units with leaky_relu activation. The critic network has  an initial dimension the size of the state size plus action size.\n",
    "\n",
    "## Plot of Rewards\n",
    "\n",
    "The results on the single-agent training are depicted below:\n",
    "\n",
    "<div>\n",
    "<img src=\"single_agent_scores.png\" width=\"300\" align=\"middle\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "```\n",
    "Episode 691\tScore: 26.59\t\n",
    "Episode 692\tScore: 30.01\t\n",
    "Episode 693\tScore: 34.65\t\n",
    "Episode 694\tScore: 27.09\t\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Using the saved checkpoints from the single-agent training, the results on the multi-agent training are derived as follows:\n",
    "\n",
    "<div>\n",
    "<img src=\"multi_agent_scores.png\" width=\"300\" align=\"middle\"/>\n",
    "</div>\n",
    "\n",
    "```\n",
    "Episode 88\tScore: 28.16\t\n",
    "Episode 89\tScore: 28.85\t\n",
    "Episode 90\tScore: 31.68\t\n",
    "Episode 91\tScore: 31.07\t\n",
    "Episode 92\tScore: 29.30\t\n",
    "Episode 93\tScore: 30.74\t\n",
    "Episode 94\tScore: 27.58\t\n",
    "Episode 95\tScore: 29.41\t\n",
    "Episode 96\tScore: 29.90\t\n",
    "Episode 97\tScore: 28.79\t\n",
    "Episode 98\tScore: 29.52\t\n",
    "Episode 99\tScore: 30.59\t\n",
    "Episode 100\tScore: 31.54\t\t\n",
    "\n",
    "```\n",
    "\n",
    "The weight initialization from the single-agent environment acted as a good initilization for the multi-agent environment. The multi-agent environment has reached a score of +30 and shows a stable behavior onwards with minor oscillations around the goal. \n",
    "\n",
    "\n",
    "## Ideas for Future Work\n",
    "Here are few options for the future work:\n",
    "\n",
    "- Proximal Policy Optimization (PPO), Distributed Distributional Deterministic Policy Gradients (D4PG), and A2C methods can be deployed.\n",
    "\n",
    "- The algorithms can be applied to the pixel version of the sinlg-agent and the multi-agent Unity environment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
