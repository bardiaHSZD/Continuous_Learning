{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "### Introduction\n",
    "\n",
    "For this project, you will work with the [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) environment.\n",
    "\n",
    "![SegmentLocal](robotic_arms.gif \"segment\")\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project utilised the DDPG (Deep Deterministic Policy Gradient) architecture outlined in the [DDPG-Bipedal Udacity project repo](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal).\n",
    "\n",
    "## State and Action Spaces\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector must be a number between -1 and 1.\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "The agent training utilised the `ddpg` function in the `Continuous_Control` notebook. It continues episodical training via the the ddpg agent until `n_episoses` is reached or until the environment tis solved. As above, a reward of +0.1 is provided for each step that the agent's hand is in the goal location. The DDPG agent is contained in `ddpg_agent.py`.\n",
    "\n",
    "Recent implementation of “Deep Q Network” (DQN) algorithm has acheived a signficant progress in Reinforcement Learning, resulting in human level performance in playing Atari games. DQN is capable of solving problems with high-dimensional state space but faces limitations in dealing with high-dimensional continous action space and requires iterative optimazation process at each step.\n",
    "\n",
    "The DDPG agents used in this experiment consists of four deep neural networks, i.e. actor_local, actor_target, critic_local, and critic_target. It starts by taking actions in epsilon-greedy manner and adding tuple of <state, action, reward, next_state, done> to its replay buffer. At every N_TIME_STEPS (e.g. 20) steps (i.e. update_rate) it does a learning process N_LEARN_UPDATES (e.g. 10) times, by updating its local actor and critic networks; this includes backpropagation steps through each network to calculate gradients and finally applying a soft update to the target networks.\n",
    "\n",
    "For the exploration noise, an Ornstein-Uhlenbeck process noise with mu = 0.0, theta = 0.15, and sigma=0.2 is implemented. Temporally correlated noise are added to actions in order to explore well in physical environments with momentum. \n",
    "\n",
    "### DDPG Hyper Parameters\n",
    "\n",
    "Hyperparameters used are:\n",
    "\n",
    "- Replay buffer size = 1e6\n",
    "- Minibatch size = 128\n",
    "- Discount factor = 0.99\n",
    "- Tau for soft update of target parameters = 1e-3\n",
    "- Learning rate of the actor = 1e-4\n",
    "- Learning rate of the critic = 1e-4\n",
    "- L2 weight decay = 0\n",
    "- Number of episodes = 300 \n",
    "- Maximum time per epsode = 1000\n",
    "- Number of agents = 1 (or 20 based on the chosen environment)\n",
    "- Number of actions = 4\n",
    "- Number of states = 33\n",
    "- Number of learn updates in memory samples = 10\n",
    "- Number of time steps required to start learning again = 20\n",
    "\n",
    "Actor and Critic network models were defined in `model.py`. The Actor networks utilised two fully connected layers with 256 and 128 units with relu activation and tanh activation for the action space. The network has an initial dimension the same as the state size. The Critic networks utilised two fully connected layers with 256+action_size and 128 units with leaky_relu activation. The critic network has  an initial dimension the size of the state size plus action size. Both architectures use learning rate of 1e-4 and Adam optimizer.\n",
    "\n",
    "\n",
    "## Plot of Rewards\n",
    "\n",
    "The results on the single-agent training are depicted below:\n",
    "\n",
    "<div>\n",
    "<img src=\"single_agent_scores.png\" width=\"300\" align=\"middle\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "```\n",
    "Episode 691\tScore: 26.59\t\n",
    "Episode 692\tScore: 30.01\t\n",
    "Episode 693\tScore: 34.65\t\n",
    "Episode 694\tScore: 27.09\t\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Using the saved checkpoints from the single-agent training, the results on the multi-agent training are derived as follows:\n",
    "\n",
    "<div>\n",
    "<img src=\"multi_agent_scores.png\" width=\"300\" align=\"middle\"/>\n",
    "</div>\n",
    "\n",
    "```\n",
    "Episode 10\tScore: 29.83\tAverage Score: 17.9095\n",
    "Episode 20\tScore: 28.76\tAverage Score: 23.9681\n",
    "Episode 30\tScore: 29.94\tAverage Score: 25.7205\n",
    "Episode 40\tScore: 30.67\tAverage Score: 27.0211\n",
    "Episode 50\tScore: 32.69\tAverage Score: 27.9301\n",
    "Episode 60\tScore: 30.33\tAverage Score: 28.4783\n",
    "Episode 70\tScore: 29.80\tAverage Score: 28.6604\n",
    "Episode 80\tScore: 27.12\tAverage Score: 28.5960\n",
    "Episode 90\tScore: 29.85\tAverage Score: 28.6900\n",
    "Episode 100\tAverage Score: 28.663.42\tmax: 32.81\n",
    "Episode 100\tScore: 27.68\tAverage Score: 28.66\n",
    "Episode 110\tScore: 28.82\tAverage Score: 29.7561\n",
    "Episode 120\tScore: 31.19\tAverage Score: 29.9472\n",
    "Timestep 999\tScore: 30.32\tmin: 24.88\tmax: 36.12\n",
    "Environment solved in 26 episodes!\tAverage Score: 30.00\t\n",
    "\n",
    "```\n",
    "\n",
    "The weight initialization from the single-agent environment acted as a good initilization for the multi-agent environment. The multi-agent environment has reached an average score of 30 and a max of 36.12 over 100 episodes, and shows a stable behavior onwards with minor oscillations around the goal. \n",
    "\n",
    "\n",
    "## Ideas for Future Work\n",
    "Here are few options for the future work:\n",
    "\n",
    "- Proximal Policy Optimization (PPO), Distributed Distributional Deterministic Policy Gradients (D4PG), and A2C methods can be deployed.\n",
    "\n",
    "- The algorithms can be applied to the pixel version of the sinlg-agent and the multi-agent Unity environment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
